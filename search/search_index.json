{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Welcome to Your AI Superpower Toolkit \ud83e\udd16\u2728   Mastering LLMs: A Deep Dive into Prompt Engineering, Agents, RAGs, and No-Code  <p> This guide offers a mix of tutorials, resources, and insights on prompt engineering, AI agents, retrieval-augmented generation (RAG), and no-code tools. Whether you're just starting or looking to expand your knowledge, you'll find useful links, research papers, and practical examples to help you navigate the evolving AI landscape. \ud83d\ude80 </p> <p> Start Learning </p> <p>This course is fully open source \ud83d\udcd6, meaning anyone can access and review the code. We believe AI should be accessible to everyone and explained in a clear, objective manner.</p>"},{"location":"Docs/LLM/Inference_and_Serving/","title":"Inference and Serving","text":""},{"location":"Docs/LLM/Inference_and_Serving/#google-colab-notebook-huggingface","title":"Google Colab Notebook + HuggingFace","text":"<p>If you just want the clean notebook you can follow this link</p> <p>Colab Runtime</p> <p>Before running the code, ensure that the Google Colab runtime is set to use a GPU.</p> <p>Go to Google Colab and create new notebook</p> <p>Check if CUDA device is visible:</p> <pre><code>!nvidia-smi\n</code></pre> <p>The output should be similar to this:</p> <pre><code>+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   46C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n</code></pre> Install dependancies<pre><code>!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q -U hf_transfer\n!pip install -q -U \"huggingface_hub[cli]\"\n</code></pre> Import modules<pre><code>import torch\nimport os\nimport transformers\nimport getpass\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\nfrom IPython.display import display, Markdown, Latex\n\n\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n</code></pre> Only for gated models<pre><code>!huggingface-cli login # for gated models\n</code></pre> <p>Set a default device to run LLM</p> <pre><code>torch.set_default_device('cuda')\n</code></pre> <pre><code>model_id = \"google/gemma-3-4b-it\" # (1)\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=quantization_config,\n    low_cpu_mem_usage=True,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n    )\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel.device\n</code></pre> <ol> <li> Change this if you want a different model</li> </ol> <p>Output should be similar to this: <code>device(type='cuda', index=0)</code></p> Run inferance on the model<pre><code>chat = [\n    { \"role\": \"user\", \"content\": \"Hi there\" },\n]\nprompt = tokenizer.apply_chat_template(\n    chat, \n    tokenize=False, \n    add_generation_prompt=True\n    )\n\ninputs = tokenizer.encode(\n    prompt, \n    add_special_tokens=True, \n    return_tensors=\"pt\"\n    )\noutputs = model.generate(\n    input_ids=inputs.to(model.device),\n    max_new_tokens=100\n    )\ntext = tokenizer.decode(\n    outputs[0],\n    skip_special_tokens=True, \n    clean_up_tokenization_spaces=True\n    )\n</code></pre> <p>In a Colab Notebook you can display Markdown as shown below</p> <pre><code>display(Markdown(text))\n</code></pre> <p>For more information on using Hugging Face, please refer to the official documentation</p>"},{"location":"Docs/LLM/Inference_and_Serving/#ollama","title":"Ollama","text":"<p>There are several ways to run Ollama, allowing you to choose the option that best fits your needs. The simplest method is to run a Docker image with your preferred model and access it via Python. For additional options, please refer to the official documentation.</p> Docker with GPU support<pre><code>docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n</code></pre> <pre><code>docker exec -it ollama ollama run gemma3:4b-it-qat\n</code></pre> <p>You can either interact with the LLM directly in your terminal, with curl or use it as an API within your code, as demonstrated below.</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    base_url = 'http://host.docker.internal:11434/v1', # (1)\n    api_key='ollama', # (2)\n)\n\nresponse = client.chat.completions.create(\n  model=\"gemma3:4b-it-qat\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hi there\"},\n  ]\n)\nprint(response.choices[0].message.content)\n# Hello! How can I help you today? \ud83d\ude0a Do you have a question, need some information, or just want to chat?\n</code></pre> <ol> <li>Use localhost insted of <code>host.docker.internal</code> if you are not connecting from other Docker image</li> <li>Required, but unused</li> </ol>"},{"location":"Docs/LLM/Resources/","title":"Resources","text":""},{"location":"Docs/LLM/Resources/#local-ai","title":"Local AI","text":"<p>Comprehensive Lists of Inference Engines, User Interfaces, Platforms, and Developer Tools</p> <ul> <li>awesome-local-ai </li> <li>awesome-local-llms</li> </ul>"},{"location":"Docs/LLM/Resources/#free-api","title":"Free API","text":"<p>List of Free LLM API</p> <ul> <li>free-llm-api-resources</li> </ul>"},{"location":"Docs/Prompt_Engineering/Papers/","title":"Prompt Engineering Papers","text":""},{"location":"Docs/Prompt_Engineering/Papers/#prompting-science-report-1-prompt-engineering-is-complicated-and-contingent","title":"Prompting Science Report 1: Prompt Engineering is Complicated and Contingent","text":"<p>[arxiv] [pdf]</p> <p>A short report exploring how benchmarking and prompting strategies impact Large Language Model (LLM) performance. It highlights that there\u2019s no universal standard for benchmarks and that prompting techniques\u2014like politeness or constraints\u2014can have unpredictable effects depending on the context.</p> AI Key Takeaways <ol> <li> <p>Benchmarking Standards Vary:</p> <p>Accuracy thresholds (100%, 90%, 51%) significantly affect model evaluations. The choice depends on the application's tolerance for errors.</p> </li> <li> <p>Prompt Engineering is Unpredictable:</p> <p>Techniques like politeness and formatting yield inconsistent results. Formatting improves performance, but politeness varies by context and question.</p> </li> <li> <p>AI Response Consistency:</p> <p>Models like GPT-4o and GPT-4o-mini show variability in responses. Repeated sampling (e.g., 100 attempts/question) offers better insights than single-instance testing.</p> </li> <li> <p>Model Comparison:</p> <p>GPT-4o generally performs better than GPT-4o-mini, but differences are small and only significant at lower accuracy thresholds.</p> </li> <li> <p>Prompting Formats Matter:</p> <p>Explicitly structured prompts enhance accuracy; unformatted prompts degrade performance.</p> </li> <li> <p>Recommendations:</p> <p>Adopt repeated testing methods. Tailor benchmarks and prompts to specific applications for better results.</p> </li> </ol> <p>Conclusion:</p> <p>AI benchmarking and prompt design are context-sensitive and require methodological rigor to address variability and inconsistency effectively.</p>"},{"location":"Tutorials/Prompt_Engineering/","title":"\ud83d\udcac Prompt Engineering: Mastering AI Communication","text":""},{"location":"Tutorials/Prompt_Engineering/#welcome-to-the-world-of-prompt-engineering","title":"Welcome to the World of Prompt Engineering","text":"<p>Prompt engineering is a discipline in the field of artificial intelligence (AI) that enables users to guide the behavior of large language models (LLMs) like ChatGPT by crafting effective instructions, known as prompts. It is a critical skill for researchers, developers, and AI enthusiasts looking to optimize model behavior, enhance efficiency, and unlock new applications.</p>"},{"location":"Tutorials/Prompt_Engineering/#what-is-prompt-engineering","title":"What is Prompt Engineering?","text":"<p>At its core, prompt engineering involves designing and fine-tuning prompts \u2014 text-based inputs that guide AI models to produce accurate, relevant, and desired outputs. It serves as a crucial interface between human intent and AI understanding.</p>"},{"location":"Tutorials/Prompt_Engineering/#key-aspects-of-prompt-engineering","title":"Key Aspects of Prompt Engineering","text":"<ul> <li>Optimizing Model Performance: Well-structured prompts improve response quality, relevance, and accuracy.</li> <li>Iterative Refinement: Effective prompts often require testing, adjusting, and fine-tuning.</li> <li>Expanding AI Capabilities: Properly designed prompts can augment LLMs with external knowledge and specialized tools.</li> <li>Enhancing Safety: Thoughtful prompt engineering helps mitigate biases and improve responsible AI deployment.</li> </ul>"},{"location":"Tutorials/Prompt_Engineering/#why-is-prompt-engineering-important","title":"Why is Prompt Engineering Important?","text":"<p>Prompt engineering is the key to unlocking the full potential of LLMs. It allows users to:</p> <ul> <li>Guide Model Behavior: Obtain AI outputs tailored to specific tasks, ensuring the model's behavior aligns with user goals.</li> <li>Enhance Output Quality: Well-crafted prompts improve the accuracy, relevance, and clarity of AI-generated content.</li> <li>Unlock Creativity: For creative applications, such as content creation or brainstorming, prompt engineering helps AI explore novel ideas.</li> <li>Automate and Optimize Workflows: By refining prompts, users can save hours by automating tasks like summarizing reports, generating emails, or analyzing data.</li> <li>Enhance AI Safety and Control: Guide models to align with ethical guidelines and factual accuracy.</li> <li>Develop Advanced AI Applications: Developers and researchers use prompt engineering to design robust tools and applications powered by AI.</li> </ul>"},{"location":"Tutorials/Prompt_Engineering/#how-does-prompt-engineering-work","title":"How Does Prompt Engineering Work?","text":"<p>A prompt is simply the input text or instruction provided to an AI system, designed to elicit a specific response. </p> <p>Optimizing a Prompt for AI</p> <p>\u274c Basic Prompt: <pre><code>Summarize prompt engineering.\n</code></pre> \u2705 Better Prompt: <pre><code>Compose a concise yet insightful three-sentence summary that highlights the key advantages of\nprompt engineering, emphasizing its role in optimizing AI responses, enhancing efficiency, and\ndriving innovation in human-computer interactions.\n</code></pre></p> <p>Example prompt and model response</p> Prompt<pre><code>Compose a concise yet insightful three-sentence summary that highlights the key advantages\nof prompt engineering, emphasizing its role in optimizing AI responses, enhancing\nefficiency, and driving innovation in human-computer interactions.\n</code></pre> GPT-4oClaude 3.7 SonnetGemma 3 27b it Q6_K LLM Response<pre><code>Prompt engineering refines AI responses by optimizing input structure, ensuring more\naccurate, relevant, and context-aware outputs. This enhances efficiency by reducing the\nneed for iterative queries, saving time and resources in various applications.\nBy unlocking AI's full potential, prompt engineering fosters innovation in human-computer\ninteractions, enabling more intuitive, creative, and effective problem-solving.\n</code></pre> LLM Response<pre><code>Prompt engineering leverages carefully crafted inputs to extract optimal AI responses,\nenabling users to guide models toward desired outcomes with precision and clarity. By\napplying structured techniques and strategic language patterns, this practice significantly\nenhances workflow efficiency through reduced iteration cycles and more accurate\nfirst-attempt results. Beyond immediate productivity gains, skillful prompt engineering\ndrives innovation by unlocking novel applications of AI capabilities and establishing more\nintuitive and productive human-computer interaction paradigms.\n</code></pre> LLM Response<pre><code>Prompt engineering unlocks the full potential of AI by crafting specific inputs that elicit\ndesired, high-quality outputs \u2013 moving beyond generic responses to tailored solutions. This\noptimization dramatically increases efficiency, reducing the need for extensive\npost-processing or repeated queries while saving valuable time and resources. Ultimately,\nskillful prompt design fosters more intuitive and productive human-computer interactions,\ndriving innovation across diverse fields by enabling new applications and creative\npossibilities with AI.\n</code></pre>"},{"location":"Tutorials/Prompt_Engineering/#what-can-prompt-engineering-be-used-for","title":"What Can Prompt Engineering Be Used For?","text":"<p>Prompt engineering has an expansive range of applications. Here are just a few examples:</p> <ol> <li>Text Generation: Automate writing tasks, including emails, essays, and creative storytelling.</li> <li>Problem-Solving: Answer questions, generate solutions, or analyze large datasets with AI assistance.</li> <li>Education and Training: Help learners understand complex concepts by phrasing questions and instructions creatively.</li> <li>Business Optimization: Use AI to enhance customer interactions, summarize meeting notes, or manage workflows.</li> <li>Creative Exploration: Inspire new ideas in design, writing, or even recipe creation (e.g., crafting a fusion of Ethiopian and Uzbek cuisine!).</li> </ol>"},{"location":"Tutorials/Prompt_Engineering/#the-evolution-of-prompt-engineering","title":"The Evolution of Prompt Engineering","text":"<p>While prompt engineering is a relatively new concept, it has grown rapidly alongside advances in LLMs like GPT-4. Over time, models have become more adept at understanding nuanced prompts and producing sophisticated outputs. This makes the discipline of constructing prompts even more critical as AI becomes more integrated into our professional and personal lives.</p>"},{"location":"Tutorials/Prompt_Engineering/#why-should-you-care-about-prompt-engineering","title":"Why Should You Care About Prompt Engineering?","text":"<p>In the fast-evolving landscape of AI, mastery of prompt engineering empowers anyone to maximize productivity, solve problems innovatively, and stay ahead in a world where AI is rapidly transforming industries. </p> <p>Recent research shows that small changes in how you phrase or format a prompt such as specific formatting can significantly impact the quality and accuracy of AI responses.</p> <p>Quote</p> <p>Techniques like politeness and formatting yield inconsistent results. Formatting improves performance, but politeness varies by context and question.</p> <p>Quote</p> <p>Explicitly structured prompts enhance accuracy; unformatted prompts degrade performance. </p> <p>Understanding these nuances enables you to craft prompts that are more reliable and aligned with your goals, especially in high-stakes or precision-critical tasks. As models can behave inconsistently and benchmarking is highly context-dependent, prompt engineering becomes not just a creative skill but a strategic one\u2014helping you unlock more consistent, useful, and impactful outcomes from AI.</p> Image by XKCD"},{"location":"Tutorials/Prompt_Engineering/For_Everyone/","title":"Prompt Engineering for Everyone","text":"<p>You're about to embark on an exciting journey into the field of prompt engineering. In recent times, powerful AI tools like ChatGPT have captured the world's imagination. Often discussed in terms of controversy or simple automation, these Large Language Models (LLMs) represent something much more profound: a new fundamental medium for creativity and problem-solving.</p> <p>This tutorial aims to reshape your perspective, showing you how these AI systems can become powerful collaborators, helping you unlock creativity, explore ideas, and bring your thoughts to life faster than ever before. We'll move beyond the hype and teach you the practical skills needed to communicate effectively with AI.</p> <p>Info</p> <p>The core of this interaction lies in the prompt: the instruction, question, or input you provide to an AI model to get a desired output. Think of it as learning the language to converse effectively with these incredibly capable tools.</p>"},{"location":"Tutorials/Prompt_Engineering/For_Everyone/#who-is-this-tutortial-for","title":"Who is This Tutortial For?","text":"<p>This tutorial is designed with beginners in mind and is perfect for a wide range of individuals. If you fit into any of these categories, you're in the right place:</p> <ul> <li> <p>Beginners curious about Artificial Intelligence and how to interact with it effectively.</p> </li> <li> <p>Professionals looking to integrate AI tools into their daily workflow to boost productivity and creativity.</p> </li> <li> <p>Educators interested in understanding and teaching about AI's capabilities and implications.</p> </li> <li> <p>Creative Individuals (writers, artists, designers, etc.) wanting to use AI as a collaborative partner.</p> </li> <li> <p>Anyone interested in improving their communication skills with AI systems to get better, more reliable results.</p> </li> </ul> <p>Essentially, if you're interested in leveraging the power of modern AI, this tutorial is for you!</p> <p>Do I Need Coding Experience?</p> <p>Absolutely not! No programming or coding knowledge is required.</p> <p>We focus on crafting effective prompts using natural language \u2013 the way you already communicate You don't need to be a data scientist or machine learning engineer. Everyone can write a prompt!</p>"},{"location":"Tutorials/Prompt_Engineering/For_Everyone/#while-a-technical-background-isnt-needed-a-few-things-will-be-helpful","title":"While a technical background isn't needed, a few things will be helpful:","text":"<ul> <li>A curious mindset, creativity, and an openness to exploring new technology.</li> <li>Basic computer familiarity and somthing that Michael Hartl calls a technical sohistication - a certain level of comfort with using technology, the ability to independently search for answers and solve minor technical problems.</li> </ul> Technical Sophistication / XKCD <p>The most crucial elements are your creativity and willingness to experiment.</p>"},{"location":"Tutorials/Prompt_Engineering/For_Everyone/getting_started/","title":"Getting Your First LLM","text":"<p>The most effective way to learn prompt engineering is through hands-on experimentation. This guide focuses on practical methods to interact with large language models (LLMs), whether locally hosted on your hardware or via cloud-based providers. Below, we explore the ecosystem of tools, platforms, and interfaces available to start engaging with LLMs immediately.</p> <p>Don't worry</p> <p>No need to worry if some of the terms are unfamiliar \u2014 we'll explain them in detail as we go along.</p>"},{"location":"Tutorials/Prompt_Engineering/For_Everyone/getting_started/#local-vs-hosted-models","title":"Local vs. Hosted Models","text":""},{"location":"Tutorials/Prompt_Engineering/For_Everyone/getting_started/#local-models","title":"Local Models","text":"<p>Local models run entirely on your hardware, offering full control over data privacy and customization. Tools like Ollama and LM Studio simplify local deployment by abstracting complex configurations, allowing users to download and run quantized models (e.g., Mistral-7B, Llama3-8B) with minimal setup. For example, Ollama\u2019s Docker-inspired CLI enables one-line model pulls (ollama run llama2), while LM Studio provides a graphical interface for model management and experimentation.</p> <p>Local deployment is ideal for sensitive applications, as no data leaves your device. However, hardware limitations such as VRAM for GPU offloading or RAM for CPU inference constrain model size and speed. For instance, a 7B-parameter model requires at least 8GB of RAM, while 70B models demand high-end GPUs or optimized quantization.</p>"},{"location":"Tutorials/Prompt_Engineering/For_Everyone/getting_started/#hosted-models","title":"Hosted Models","text":"<p>Hosted models, offered by providers like OpenAI, Anthropic, and Google, abstract infrastructure management, allowing users to access state-of-the-art models (e.g., GPT-4, Claude 3.7) via APIs or UI (graphical interface). These services excel in ease of use and scalability, as they handle hardware optimization, updates, and latency.</p> <p>However, reliance on third-party APIs or UIs introduces privacy risks, as queries and outputs may be logged or used for training. Costs can also escalate with high usage or pro subscriptions.</p>"},{"location":"Tutorials/Prompt_Engineering/For_Everyone/getting_started/#ui-options-for-interacting-with-llms","title":"UI Options for Interacting with LLMs","text":"<p>Beginner-friendly UIs like LM Studio and Jan offer intuitive graphical environments for local model interaction. These tools support features such as:</p> <ul> <li>Model browsing and downloading from repositories like Hugging Face.</li> <li>Chat interfaces with persistent conversation histories.</li> <li>Parameter tuning (temperature, top-p sampling) for output customization.</li> </ul> <p>For collaborative or multi-user scenarios, Open WebUI provides a self-hosted web interface with user authentication and document retrieval (RAG) capabilities. Meanwhile, SillyTavern and AnythingLLM cater to advanced users, offering plugin systems for image generation, web searching, and API integrations.</p> <p>Warning</p> <p>There are some concerns associated with certain locally hosted UIs. For example, while LM Studio does not appear to connect to the internet after downloading a model and explicitly states on its homepage, \"The app does not collect data or monitor your actions. Your data stays local on your machine\" questions have been raised regarding its terms of use, which may warrant careful review before adoption.</p>"},{"location":"Tutorials/Prompt_Engineering/For_Everyone/getting_started/#conclusion-choosing-your-starting-point","title":"Conclusion: Choosing Your Starting Point","text":"<p>For beginners, hosted models lower the barrier to entry by eliminating setup complexity. Platforms like OpenAI provide free tiers for experimentation. As proficiency grows, transitioning to local models via Ollama or LM Studio offers greater control and cost savings for specialized tasks.</p> <p>Choosing UI</p> <p>If you'd like to follow along with lessons such as parameter tuning for LLMs, we recommend selecting an environment that supports this functionality. Some free-tier hosted model providers may limit or obscure access to such features. While we\u2019ll offer some recommendations, please note that these are current as of the time of writing, and you should verify their capabilities independently.</p>"},{"location":"Tutorials/Prompt_Engineering/For_Everyone/getting_started/#recommendations","title":"Recommendations","text":"<p>The following options are presented in order of increasing complexity, starting with those that are the easiest to set up and progressing to more advanced configurations.</p>"},{"location":"Tutorials/Prompt_Engineering/For_Everyone/getting_started/#hosted-free-ui","title":"Hosted Free UI","text":"<p>The simplest option is to use a hosted, free user interface, with platforms like ChatGPT and Claude being popular choices. However, as of this writing, access to more advanced features is typically limited to paid users. For those looking to explore these capabilities at no cost, we recommend using Google AI Studio, which provides broad access to advanced options within a free tier.</p> Google AI Studio"},{"location":"Tutorials/Prompt_Engineering/For_Everyone/getting_started/#self-hosted-llm-with-ui","title":"Self-Hosted LLM with UI","text":"<p>Requirements</p> <p>This option requires a high-performance machine, ideally equipped with a GPU. While it is possible to run smaller models using quantization, which can significantly improve speed and reduce resource requirements, it's important to note that the quality of the model's responses may be lower.</p> <p>Visit the LM Studio website and download the version that is compatible with your operating system. Once installed, launch the application. </p> LM Studio <p>Please note that it will not function immediately, as it requires a model to run. To get started, navigate to the  \"Discover\" tab (on the left side of the app) and select a model and quantization option for example <code>Q4_K_M</code> to download. For starting, we recommend a smaller model such as <code>Mistral-7B-Instruct-v0.3-GGUF</code>, which balances performance and resource requirements. If you don't have access to a GPU but still wish to run a model locally, you can try a lightweight option such as <code>Gemma 2B</code> or <code>Gemma 4B QAT</code> (Quantization Aware Training). These models are designed to be more efficient and can run on CPU-only systems, though performance and response speed may vary depending on your hardware.</p> Search &amp; Download model in LM Studio <p>After the download is complete, you can go to  Chat and load the model (dropdown at the top of the chat screen saying \"Select a model to load\")</p> Loading Model <p>and begin interacting with it.</p> Interacting with the LLM <p>After you are finished remember to eject the model and close the app.</p>"},{"location":"Tutorials/Prompt_Engineering/For_Everyone/getting_started/#ui-with-openrouter-free-and-paid-llms","title":"UI with OpenRouter Free and Paid LLMs","text":"<p>To use Jan.ai (or a similar platform like AnythingLLM) with OpenRouter, a few additional setup steps are required:</p> <ul> <li>Download and install Jan.ai from its official website.</li> <li>Register for an account at OpenRouter.</li> <li>Once logged in, navigate to Settings \u2192 API Keys on the OpenRouter website.</li> <li>Generate a new API key and copy it immediately. It will look something like: <code>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</code>.</li> </ul> <p>API Key</p> <p>You can only view the key once, so if you forget to copy it, simply generate a new one.</p> <ul> <li>In the Jan.ai application, go to Settings \u2192 Remote Engine \u2192 OpenRouter, and paste your API key into the appropriate field.</li> <li>You can now start a new thread and select a model by clicking the model selector at the bottom of the chat window. Choose Cloud \u2192 type \u201cfree\u201d, then scroll to locate OpenRouter and select your model. You need to check terms &amp; conditions on the model since some free models are avaliable only to users that made a purchase.</li> </ul> <p>Free model</p> <p>For example, you can select: <code>nvidia/llama-3.3-nemotron-super-9b-v1:free</code></p> <p>Optional \u2013 Using Google Models for Free: If you prefer using Google hosted models, you can obtain a free API key by clicking \"Get API Key\" at the top of the Google AI Studio interface and following the same steps. Just be sure to choose Google as the provider instead of OpenRouter.</p> Jan"},{"location":"Tutorials/Prompt_Engineering/For_Everyone/getting_started/#local-options-for-advanced-users","title":"Local Options for Advanced Users","text":"<p>If you want to use an LLM directly from the terminal, with tools like cURL or from a programming language such as Python, you can explore additional methods in the Inference and Serving section.</p>"},{"location":"Tutorials/Prompt_Engineering/For_Everyone/getting_started/#more-options","title":"More Options","text":"<p>You can check more UI, inference engines, etc. options and free LLM API like OpenRouter in LLM Resources</p>"},{"location":"Tutorials/Prompt_Engineering/For_Everyone/getting_started/#what-next","title":"What next?","text":"<p>Experiment with the LLM you\u2019ve just installed, then continue to the next section on Prompt Engineering.</p>"},{"location":"Tutorials/Prompt_Engineering/For_Everyone/getting_started/#bonus","title":"Bonus","text":"<p>If you're feeling nostalgic and looking to add a bit of fun to your experience, consider installing Clippy and interacting with it through an interface. Enjoy!</p> Clippy in action"},{"location":"pl/","title":"Strona G\u0142\u00f3wna","text":"Odkryj swoj\u0105 supermoc z pomoc\u0105 sztucznej inteligencji! \ud83e\udd16\u2728   Opanuj LLM'y: Wprowadzenie do Prompt Engineering, Agent\u00f3w, RAG i No-Code  <p> Ten przewodnik to zbi\u00f3r tutoriali, materia\u0142\u00f3w i ciekawych zasob\u00f3w na temat in\u017cynierii prompt\u00f3w, agent\u00f3w AI, retrieval-augmented generation (RAG) oraz narz\u0119dzi no-code. Niezale\u017cnie od tego, czy dopiero zaczynasz, czy chcesz poszerzy\u0107 swoj\u0105 wiedz\u0119, znajdziesz tu przydatne linki, artyku\u0142y naukowe i praktyczne przyk\u0142ady, kt\u00f3re pomog\u0105 Ci lepiej zrozumie\u0107 \u015bwiat AI. \ud83d\ude80 </p> <p> Zaczynajmy </p> <p>Ten kurs jest w pe\u0142ni open source \ud83d\udcd6, co oznacza, \u017ce ka\u017cdy mo\u017ce uzyska\u0107 dost\u0119p do kodu i go przejrze\u0107. Wierzymy, \u017ce sztuczna inteligencja powinna by\u0107 dost\u0119pna dla wszystkich i wyja\u015bniana w spos\u00f3b jasny oraz obiektywny.</p>"},{"location":"pl/Docs/LLM/Inference_and_Serving/#google-colab-notebook-huggingface","title":"Google Colab Notebook + HuggingFace","text":"<p>Je\u015bli chcesz tylko czysty notebook, skorzystaj z tego linku</p> <p>Colab Runtime</p> <p>Przed uruchomieniem kodu upewnij si\u0119, \u017ce \u015brodowisko wykonawcze Google Colab jest ustawione aby korzysta\u0107 z akceleratora sprzetowego (np. GPU T4).</p> <p>Przejd\u017a do Google Colab i stw\u00f3rz nowy notebook</p> <p>Sprawd\u017a czy dost\u0119pne jest GPU:</p> <pre><code>!nvidia-smi\n</code></pre> <p>Wynik wykonania powy\u017cszego polecenia powinien wygl\u0105da\u0107 podobnie do tego</p> <pre><code>+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   46C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n</code></pre> Zainstaluj paczki<pre><code>!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q -U hf_transfer\n!pip install -q -U \"huggingface_hub[cli]\"\n</code></pre> Zaimportuj modu\u0142y<pre><code>import torch\nimport os\nimport transformers\nimport getpass\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\nfrom IPython.display import display, Markdown, Latex\n\n\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n</code></pre> Tylko dla gated models<pre><code>!huggingface-cli login # dla gated models\n</code></pre> <p>Ustaw domy\u015blne urz\u0105dzenie do inferencji</p> <pre><code>torch.set_default_device('cuda')\n</code></pre> <pre><code>model_id = \"speakleash/Bielik-4.5B-v3.0-Instruct\" # (1)\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=quantization_config,\n    low_cpu_mem_usage=True,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n    )\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel.device\n</code></pre> <ol> <li> Zmie\u0144 t\u0105 linijk\u0119 je\u017celi chcesz skorzysta\u0107 z innego modelu</li> </ol> <p>Wynik powy\u017cszego powinien wygl\u0105da\u0107 nast\u0119puj\u0105co: <code>device(type='cuda', index=0)</code></p> Wywo\u0142aj model<pre><code>chat = [\n    {\"role\": \"system\", \"content\": \"Odpowiadaj kr\u00f3tko, precyzyjnie i wy\u0142\u0105cznie w j\u0119zyku polskim.\"},\n    { \"role\": \"user\", \"content\": \"Cze\u015b\u0107\" },\n]\n\nprompt = tokenizer.apply_chat_template(\n    chat,\n    tokenize=False,\n    add_generation_prompt=True\n    )\n\ninputs = tokenizer.encode(\n    prompt, \n    add_special_tokens=True, \n    return_tensors=\"pt\"\n    )\noutputs = model.generate(\n    input_ids=inputs.to(model.device),\n    max_new_tokens=100\n    )\ntext = tokenizer.decode(\n    outputs[0],\n    skip_special_tokens=True, \n    clean_up_tokenization_spaces=True\n    )\n</code></pre> <p>W notebooku Colab mo\u017cesz wy\u015bwietla\u0107 Markdown, jak poni\u017cej</p> <pre><code>display(Markdown(text))\n</code></pre> <p>Aby uzyska\u0107 wi\u0119cej informacji na temat korzystania z Hugging Face, zapoznaj si\u0119 z oficjaln\u0105 dokumentacj\u0105</p>"},{"location":"pl/Docs/LLM/Inference_and_Serving/#ollama","title":"Ollama","text":"<p>Istnieje kilka sposob\u00f3w uruchomienia Ollama, dzi\u0119ki czemu mo\u017cesz wybra\u0107 opcj\u0119 najlepiej odpowiadaj\u0105c\u0105 Twoim potrzebom. Najprostsza metoda to uruchomienie obrazu Dockera z wybranym modelem i dost\u0119p do niego za pomoc\u0105 Pythona. Po dodatkowe opcje zapoznaj si\u0119 z oficjaln\u0105 dokumentacj\u0105.</p> Docker z GPU<pre><code>docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n</code></pre> <pre><code>docker exec -it ollama ollama run SpeakLeash/bielik-4.5b-v3.0-instruct:Q8_0\n</code></pre> <p>Mo\u017cesz bezpo\u015brednio z terminala wywo\u0142a\u0107 LLM, u\u017cywaj\u0105c cURL, lub korzysta\u0107 z niego jako API w swoim kodzie, jak pokazano poni\u017cej:</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    base_url = 'http://host.docker.internal:11434/v1', # (1)\n    api_key='ollama', # (2)\n)\n\nresponse = client.chat.completions.create(\n  model=\"SpeakLeash/bielik-4.5b-v3.0-instruct:Q8_0\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"Odpowiadaj kr\u00f3tko, precyzyjnie i wy\u0142\u0105cznie w j\u0119zyku polskim.\"},\n    {\"role\": \"user\", \"content\": \"Cze\u015b\u0107\"},\n  ]\n)\nprint(response.choices[0].message.content)\n# Cze\u015b\u0107! Jak mog\u0119 Ci pom\u00f3c?\n</code></pre> <ol> <li>U\u017cyj localhost zamiast <code>host.docker.internal</code> je\u017celi nie \u0142\u0105czysz si\u0119 z innego obrazu Dockerowego</li> <li>Wymagana, ale nie u\u017cywane</li> </ol>"},{"location":"pl/Docs/LLM/Resources/","title":"Materia\u0142y","text":""},{"location":"pl/Docs/LLM/Resources/#loakalne-ai","title":"Loakalne AI","text":"<p>Kompleksowe zestawienia silnik\u00f3w inferencyjnych, interfejs\u00f3w u\u017cytkownika, platform i narz\u0119dzi deweloperskich</p> <p>awesome-local-ai awesome-local-llms</p>"},{"location":"pl/Docs/LLM/Resources/#darmowe-api-dla-llm","title":"Darmowe API dla LLM","text":"<p>Lista darmowych API</p> <p>free-llm-api-resources</p>"},{"location":"pl/Docs/Prompt_Engineering/Papers/","title":"Prompt Engineering Papery","text":""},{"location":"pl/Docs/Prompt_Engineering/Papers/#prompting-science-report-1-prompt-engineering-is-complicated-and-contingent","title":"Prompting Science Report 1: Prompt Engineering is Complicated and Contingent","text":"<p>[arxiv] [pdf]</p> <p>Kr\u00f3tki raport badaj\u0105cy, jak strategie benchamrkowania i promptowania wp\u0142ywaj\u0105 na wydajno\u015b\u0107 du\u017cych modeli j\u0119zykowych (LLM). Podkre\u015bla, \u017ce nie istnieje uniwersalny standard benchmark\u00f3w, a techniki promptowania \u2014 takie jak uprzejmo\u015b\u0107 czy ograniczenia \u2014 mog\u0105 mie\u0107 nieprzewidywalne skutki w zale\u017cno\u015bci od kontekstu.</p> AI Kluczowe wnioski <ol> <li> <p>Standardy benchmark\u00f3w s\u0105 zr\u00f3\u017cnicowane:</p> <p>Progi dok\u0142adno\u015bci (100%, 90%, 51%) znacz\u0105co wp\u0142ywaj\u0105 na ocen\u0119 modeli. Wyb\u00f3r zale\u017cy od tolerancji aplikacji na b\u0142\u0119dy.</p> </li> <li> <p>In\u017cynieria prompt\u00f3w jest nieprzewidywalna:</p> <p>Techniki takie jak uprzejmo\u015b\u0107 i formatowanie daj\u0105 niesp\u00f3jne rezultaty. Formatowanie zwykle poprawia wydajno\u015b\u0107, natomiast wp\u0142yw uprzejmo\u015bci zale\u017cy od kontekstu i pytania.</p> </li> <li> <p>Sp\u00f3jno\u015b\u0107 odpowiedzi AI:</p> <p>Modele takie jak GPT-4o i GPT-4o-mini wykazuj\u0105 zmienno\u015b\u0107 w odpowiedziach. Powtarzalne pr\u00f3bkowanie (np. 100 pr\u00f3b na pytanie) daje lepszy wgl\u0105d ni\u017c testowanie pojedy\u0144czych pyta\u0144.</p> </li> <li> <p>Por\u00f3wnanie modeli:</p> <p>GPT-4o generalnie wypada lepiej ni\u017c GPT-4o-mini, jednak r\u00f3\u017cnice s\u0105 niewielkie i istotne g\u0142\u00f3wnie przy ni\u017cszych progach dok\u0142adno\u015bci.</p> </li> <li> <p>Format prompt\u00f3w ma znaczenie:</p> <p>Wyra\u017anie ustrukturyzowane prompty zwi\u0119kszaj\u0105 dok\u0142adno\u015b\u0107, nieformatowane obni\u017caj\u0105.</p> </li> <li> <p>Rekomendacje:</p> <p>Testuj wielokrotnie. Dostosuj benchmarki i prompty do konkretnych zastosowa\u0144, aby uzyska\u0107 lepsze wyniki.</p> </li> </ol> <p>Wnioski:</p> <p>Benchmarki i projektowanie prompt\u00f3w dla AI s\u0105 zale\u017cne od kontekstu i wymagaj\u0105 metodycznego podej\u015bcia, aby skutecznie radzi\u0107 sobie ze zmienno\u015bci\u0105 i niesp\u00f3jno\u015bci\u0105.</p>"},{"location":"pl/Tutorials/Prompt_Engineering/","title":"\ud83d\udcac In\u017cynieria Prompt\u00f3w: Mistrzostwo w Komunikacji z AI","text":""},{"location":"pl/Tutorials/Prompt_Engineering/#witaj-w-swiecie-inzynierii-promptow-prompt-engineering","title":"Witaj w \u015bwiecie in\u017cynierii prompt\u00f3w (Prompt Engineering)","text":"<p>In\u017cynieria prompt\u00f3w to dyscyplina w dziedzinie sztucznej inteligencji (AI), kt\u00f3ra pozwala u\u017cytkownikom kierowa\u0107 zachowaniem du\u017cych modeli j\u0119zykowych (LLM), takich jak ChatGPT, poprzez tworzenie skutecznych instrukcji, zwanych promptami. Jest to kluczowa umiej\u0119tno\u015b\u0107 dla badaczy, deweloper\u00f3w i entuzjast\u00f3w AI, kt\u00f3rzy chc\u0105 optymalizowa\u0107 dzia\u0142anie modeli, zwi\u0119ksza\u0107 ich efektywno\u015b\u0107 i odkrywa\u0107 nowe zastosowania.</p>"},{"location":"pl/Tutorials/Prompt_Engineering/#czym-jest-inzynieria-promptow","title":"Czym jest in\u017cynieria prompt\u00f3w?","text":"<p>W swojej istocie in\u017cynieria prompt\u00f3w polega na projektowaniu i dostosowywaniu prompt\u00f3w \u2014 tekstowych polece\u0144, kt\u00f3re kieruj\u0105 modelami AI, aby generowa\u0142y precyzyjne, trafne i po\u017c\u0105dane odpowiedzi. Stanowi kluczowy interfejs mi\u0119dzy ludzkimi intencjami a rozumieniem AI.</p>"},{"location":"pl/Tutorials/Prompt_Engineering/#kluczowe-aspekty-inzynierii-promptow-prompt-engineering","title":"Kluczowe Aspekty in\u017cynierii prompt\u00f3w (Prompt Engineering)","text":"<ul> <li>Optymalizacja Wydajno\u015bci Modelu: Dobrze skonstruowane polecenia poprawiaj\u0105 jako\u015b\u0107, trafno\u015b\u0107 oraz precyzj\u0119 odpowiedzi.  </li> <li>Iteracyjne Udoskonalanie: Skuteczne polecenia cz\u0119sto wymagaj\u0105 testowania, dostosowywania i dopracowywania.  </li> <li>Rozszerzanie Mo\u017cliwo\u015bci Sztucznej Inteligencji: Odpowiednio zaprojektowane polecenia mog\u0105 wzbogaci\u0107 modele j\u0119zykowe o zewn\u0119trzn\u0105 wiedz\u0119 i wyspecjalizowane narz\u0119dzia.  </li> <li>Zwi\u0119kszanie Bezpiecze\u0144stwa: Przemy\u015blany prompt engineering pomaga ogranicza\u0107 uprzedzenia i wspiera\u0107 odpowiedzialne wdra\u017canie sztucznej inteligencji.</li> </ul>"},{"location":"pl/Tutorials/Prompt_Engineering/#dlaczego-inzynieria-promptow-jest-wazna","title":"Dlaczego In\u017cynieria Prompt\u00f3w Jest Wa\u017cna?","text":"<p>In\u017cynieria promtp\u00f3w (Prompt Engineering) jest kluczem do odblokowania pe\u0142nego potencja\u0142u modeli j\u0119zykowych (LLMs). Umo\u017cliwia u\u017cytkownikom:</p> <ul> <li>Ukierunkowanie Zachowania Modelu: Uzyskiwanie wynik\u00f3w AI dostosowanych do konkretnych zada\u0144, zapewniaj\u0105c, \u017ce dzia\u0142anie modelu jest zgodne z celami u\u017cytkownika.  </li> <li>Poprawa Jako\u015bci Wynik\u00f3w: Dobrze opracowane polecenia zwi\u0119kszaj\u0105 precyzj\u0119, trafno\u015b\u0107 i jasno\u015b\u0107 generowanych tre\u015bci przez AI.  </li> <li>Uwolnienie Kreatywno\u015bci: W zastosowaniach kreatywnych, takich jak tworzenie tre\u015bci czy burza m\u00f3zg\u00f3w, prompt engineering pozwala AI eksplorowa\u0107 nowe pomys\u0142y.  </li> <li>Automatyzacja i Optymalizacja Pracy: Dzi\u0119ki doskonaleniu prompt\u00f3w u\u017cytkownicy mog\u0105 zaoszcz\u0119dzi\u0107 godziny pracy, automatyzuj\u0105c takie zadania jak podsumowywanie raport\u00f3w, generowanie e-maili czy analiza danych.  </li> <li>Zwi\u0119kszenie Bezpiecze\u0144stwa i Kontroli AI: Kierowanie modelami w celu przestrzegania zasad etycznych oraz dbania o zgodno\u015b\u0107 z faktami.  </li> <li>Tworzenie Zaawansowanych Aplikacji AI: Prompt engineering umo\u017cliwia deweloperom i badaczom projektowanie narz\u0119dzi i aplikacji opartych na sztucznej inteligencji.</li> </ul>"},{"location":"pl/Tutorials/Prompt_Engineering/#jak-dziaa-prompt-engineering","title":"Jak Dzia\u0142a Prompt Engineering?","text":"<p>Prompt to po prostu tekst wej\u015bciowy lub instrukcja dostarczona systemowi AI, zaprojektowana w celu otrzymania konkretnej odpowiedzi.</p> <p>Optimizing a Prompt for AI</p> <p>\u274c Podstawowy Prompt: <pre><code>Podsumuj temat prompt engineering.\n</code></pre> \u2705 Lepszy Prompt: <pre><code>Napisz w trzech zdaniach zwi\u0119z\u0142e, ale wnikliwe podsumowanie, kt\u00f3re podkre\u015bli kluczowe zalety\nin\u017cynierii prompt\u00f3w. Zwr\u00f3\u0107 uwag\u0119 na rol\u0119 w optymalizacji odpowiedzi AI, zwi\u0119kszaniu\nefektywno\u015bci i nap\u0119dzaniu innowacji w interakcjach cz\u0142owiek-komputer.\n</code></pre></p> <p>Przyk\u0142ad promptu i odpowiedzi modelu</p> Prompt<pre><code>Napisz w trzech zdaniach zwi\u0119z\u0142e, ale wnikliwe podsumowanie, kt\u00f3re podkre\u015bli kluczowe zalety\nin\u017cynierii prompt\u00f3w. Zwr\u00f3\u0107 uwag\u0119 na rol\u0119 w optymalizacji odpowiedzi AI, zwi\u0119kszaniu\nefektywno\u015bci i nap\u0119dzaniu innowacji w interakcjach cz\u0142owiek-komputer.\n</code></pre> Bielik 11B v2.3 Instruct Q8_0GPT-4o-miniGemini 2.0 FlashGemma 3 27b it Q6_K LLM Response<pre><code>In\u017cynieria prompt\u00f3w to prze\u0142omowa metoda optymalizuj\u0105ca odpowiedzi AI, umo\u017cliwiaj\u0105c\nprecyzyjne kierowanie modelami j\u0119zykowymi i zwi\u0119kszaj\u0105c ich efektywno\u015b\u0107. Dzi\u0119ki temu\npodej\u015bciu mo\u017cliwe jest tworzenie bardziej trafnych, spersonalizowanych interakcji mi\u0119dzy\nu\u017cytkownikami a systemami AI, co nap\u0119dza innowacje w dziedzinie HCI (Human-Computer\nInteraction). W rezultacie, in\u017cynieria prompt\u00f3w otwiera nowe mo\u017cliwo\u015bci dla naturalnych i\nintuicyjnych interakcji cz\u0142owiek-komputer.\n</code></pre> LLM Response<pre><code>In\u017cynieria prompt\u00f3w umo\u017cliwia precyzyjne formu\u0142owanie zapyta\u0144, co pozwala na uzyskiwanie\ntrafnych i skutecznych odpowiedzi od system\u00f3w AI. Poprzez optymalizacj\u0119 komunikacji mi\u0119dzy\nu\u017cytkownikiem a maszyn\u0105, proces ten zwi\u0119ksza efektywno\u015b\u0107 oraz precyzj\u0119 interakcji,\nskracaj\u0105c czas odpowiedzi i poprawiaj\u0105c jako\u015b\u0107 wynik\u00f3w. Dodatkowo, sprzyja innowacjom,\npozwalaj\u0105c na rozw\u00f3j bardziej zaawansowanych, elastycznych i intuicyjnych aplikacji, kt\u00f3re\nlepiej odpowiadaj\u0105 na z\u0142o\u017cone potrzeby u\u017cytkownik\u00f3w.\n</code></pre> LLM Response<pre><code>In\u017cynieria prompt\u00f3w to kluczowa dziedzina, kt\u00f3ra optymalizuje interakcje z modelami AI,\numo\u017cliwiaj\u0105c uzyskanie bardziej precyzyjnych i u\u017cytecznych odpowiedzi. Poprzez umiej\u0119tne\nformu\u0142owanie zapyta\u0144, in\u017cynierowie prompt\u00f3w zwi\u0119kszaj\u0105 efektywno\u015b\u0107 system\u00f3w AI, co\nprzek\u0142ada si\u0119 na lepsze wykorzystanie ich potencja\u0142u w r\u00f3\u017cnych zastosowaniach. Ponadto,\nin\u017cynieria prompt\u00f3w nap\u0119dza innowacje w interakcjach cz\u0142owiek-komputer, otwieraj\u0105c nowe\nmo\u017cliwo\u015bci w dziedzinach takich jak obs\u0142uga klienta, tworzenie tre\u015bci czy analiza danych.\n</code></pre> LLM Response<pre><code>In\u017cynieria prompt\u00f3w to dyscyplina polegaj\u0105ca na projektowaniu efektywnych instrukcji dla\nmodeli j\u0119zykowych AI, co bezpo\u015brednio przek\u0142ada si\u0119 na jako\u015b\u0107 i precyzj\u0119 generowanych\nodpowiedzi. Poprzez staranne formu\u0142owanie zapyta\u0144, in\u017cynierowie prompt\u00f3w optymalizuj\u0105\nwydajno\u015b\u0107 AI, minimalizuj\u0105c niejasno\u015bci i maksymalizuj\u0105c u\u017cyteczno\u015b\u0107 wynik\u00f3w w konkretnych\nzastosowaniach. Ta umiej\u0119tno\u015b\u0107 staje si\u0119 kluczowym elementem nap\u0119dzaj\u0105cym innowacje w\ninterakcjach cz\u0142owiek-komputer, otwieraj\u0105c nowe mo\u017cliwo\u015bci w automatyzacji zada\u0144, tworzeniu\ntre\u015bci i rozwi\u0105zywaniu problem\u00f3w.\n</code></pre>"},{"location":"pl/Tutorials/Prompt_Engineering/#do-czego-mozna-wykorzystac-inzynierie-promptow-prompt-engineering","title":"Do czego mo\u017cna wykorzysta\u0107 in\u017cynieri\u0119 prompt\u00f3w (Prompt Engineering)?","text":"<p>In\u017cynieria prompt\u00f3w ma szeroki zakres zastosowa\u0144. Oto kilka przyk\u0142ad\u00f3w:</p> <ol> <li>Generowanie tekstu: Automatyzacja zada\u0144, takich takich jak tworzenie e-maili, esej\u00f3w czy kreatywnych opowiada\u0144.</li> <li>Rozwi\u0105zywanie problem\u00f3w: Odpowiadanie na pytania, generowanie rozwi\u0105za\u0144 lub analiza du\u017cych zbior\u00f3w danych z pomoc\u0105 AI.</li> <li>Edukacja i szkolenia: Wspieranie uczni\u00f3w w zrozumieniu skomplikowanych koncept\u00f3w poprzez kreatywne formu\u0142owanie pyta\u0144 i instrukcji.</li> <li>Optymalizacja biznesowa: Wykorzystanie AI do ulepszania interakcji z klientami, podsumowywania notatek ze spotka\u0144 lub zarz\u0105dzania procesami pracy.</li> <li>Eksploracja kreatywna: Inspirowanie nowych pomys\u0142\u00f3w w dziedzinach takich jak projektowanie, pisanie czy nawet tworzenie przepis\u00f3w (np. po\u0142\u0105czenie kuchni etiopskiej i uzbeckiej!).</li> </ol>"},{"location":"pl/Tutorials/Prompt_Engineering/#ewolucja-inzynierii-promptow","title":"Ewolucja in\u017cynierii prompt\u00f3w","text":"<p>Chocia\u017c in\u017cynieria prompt\u00f3w jest stosunkowo nowym poj\u0119ciem, rozwija\u0142a si\u0119 dynamicznie wraz z post\u0119pami w modelach j\u0119zykowych opartych na uczeniu g\u0142\u0119bokim (LLM), takich jak GPT-4. W miar\u0119 up\u0142ywu czasu modele stawa\u0142y si\u0119 coraz bardziej wprawne w rozumieniu subtelnych prompt\u00f3w i generowaniu wyrafinowanych odpowiedzi. Sprawia to, \u017ce umiej\u0119tno\u015b\u0107 konstruowania promp\u00f3w staje si\u0119 co raz wa\u017cniejsza, gdy AI coraz bardziej integruje si\u0119 z naszym \u017cyciem zawodowym i prywatnym.</p>"},{"location":"pl/Tutorials/Prompt_Engineering/#dlaczego-warto-zainteresowac-sie-prompt-engineering","title":"Dlaczego warto zainteresowa\u0107 si\u0119 prompt engineering?","text":"<p>W szybko zmieniaj\u0105cym si\u0119 \u015bwiecie sztucznej inteligencji, opanowanie in\u017cynierii prompt\u00f3w pozwala ka\u017cdemu maksymalizowa\u0107 produktywno\u015b\u0107, rozwi\u0105zywa\u0107 problemy w innowacyjny spos\u00f3b i pozosta\u0107 o krok przed innymi w \u015bwiecie, gdzie AI dynamicznie przekszta\u0142ca ca\u0142e bran\u017ce.</p> <p>Najnowsze badania pokazuj\u0105, \u017ce drobne zmiany w sposobie formu\u0142owania lub formatowania prompta \u2014 takie jak konkretne formatowanie \u2014 mog\u0105 znacz\u0105co wp\u0142yn\u0105\u0107 na jako\u015b\u0107 i dok\u0142adno\u015b\u0107 odpowiedzi generowanych przez AI.</p> <p>Quote</p> <p>Techniki takie jak uprzejmo\u015b\u0107 i formatowanie daj\u0105 niesp\u00f3jne rezultaty. Formatowanie zwykle poprawia wydajno\u015b\u0107, natomiast wp\u0142yw uprzejmo\u015bci zale\u017cy od kontekstu i pytania.</p> <p>Quote</p> <p>Wyra\u017anie ustrukturyzowane prompty zwi\u0119kszaj\u0105 dok\u0142adno\u015b\u0107, nieformatowane obni\u017caj\u0105.</p> <p>Zrozumienie tych niuans\u00f3w pozwala tworzy\u0107 prompty, kt\u00f3re s\u0105 bardziej niezawodne i lepiej dopasowane do Twoich cel\u00f3w \u2014 szczeg\u00f3lnie w zadaniach wymagaj\u0105cych precyzji. Poniewa\u017c modele mog\u0105 zachowywa\u0107 si\u0119 niesp\u00f3jnie, a benchmarki s\u0105 silnie zale\u017cne od kontekstu, prompt engineering staje si\u0119 nie tylko umiej\u0119tno\u015bci\u0105 tw\u00f3rcz\u0105, ale te\u017c strategiczn\u0105 \u2014 pomagaj\u0105c uzyska\u0107 bardziej sp\u00f3jne i u\u017cyteczne rezultaty z AI.</p> Komiks od XKCD"},{"location":"pl/Tutorials/Prompt_Engineering/For_Everyone/","title":"Prompt Engineering dla ka\u017cdego","text":"<p>Wyruszasz w\u0142a\u015bnie w ekscytuj\u0105c\u0105 podr\u00f3\u017c po \u015bwiecie in\u017cynierii prompt\u00f3w. W ostatnim czasie pot\u0119\u017cne narz\u0119dzia AI, takie jak ChatGPT, rozpali\u0142y wyobra\u017ani\u0119 ludzi na ca\u0142ym \u015bwiecie. Cz\u0119sto omawiane w kontek\u015bcie kontrowersji lub automatyzacji, du\u017ce modele j\u0119zykowe (LLM) reprezentuj\u0105 co\u015b znacznie g\u0142\u0119bszego \u2014 nowy, fundamentalny spos\u00f3b kreatywno\u015bci i rozwi\u0105zywania problem\u00f3w.</p> <p>Ten kurs ma na celu zmian\u0119 Twojej perspektywy, pokazuj\u0105c, jak systemy AI mog\u0105 sta\u0107 si\u0119 pot\u0119\u017cnymi wsp\u00f3\u0142pracownikami, pomagaj\u0105c Ci odblokowa\u0107 kreatywno\u015b\u0107, eksplorowa\u0107 pomys\u0142y i urzeczywistnia\u0107 my\u015bli szybciej ni\u017c kiedykolwiek wcze\u015bniej. Wyjdziemy poza medialny szum i nauczymy Ci\u0119 praktycznych umiej\u0119tno\u015bci potrzebnych do skutecznej komunikacji z AI.</p> <p>Info</p> <p>Sednem tej interakcji jest prompt \u2014 czyli instrukcja, pytanie lub dane wej\u015bciowe, kt\u00f3re przekazujesz modelowi AI, aby uzyska\u0107 po\u017c\u0105dany rezultat. Mo\u017cna to por\u00f3wna\u0107 do nauki nowego j\u0119zyka, kt\u00f3ry pozwala skutecznie porozumiewa\u0107 si\u0119 z tymi niezwykle zaawansowanymi narz\u0119dziami.</p>"},{"location":"pl/Tutorials/Prompt_Engineering/For_Everyone/#dla-kogo-jest-ten-tutorial","title":"Dla kogo jest ten tutorial?","text":"<p>Ten tutorial zosta\u0142 stworzony z my\u015bl\u0105 o osobach pocz\u0105tkuj\u0105cych i jest idealny dla szerokiego grona odbiorc\u00f3w. Je\u015bli wpisujesz si\u0119 w kt\u00f3r\u0105kolwiek z poni\u017cszych kategorii \u2014 jeste\u015b we w\u0142a\u015bciwym miejscu:</p> <ul> <li> <p>Pocz\u0105tkuj\u0105cy, kt\u00f3rzy s\u0105 ciekawi sztucznej inteligencji i chc\u0105 nauczy\u0107 si\u0119, jak skutecznie z ni\u0105 wsp\u00f3\u0142pracowa\u0107.</p> </li> <li> <p>Profesjonali\u015bci pragn\u0105cy zintegrowa\u0107 narz\u0119dzia AI z codzienn\u0105 prac\u0105, by zwi\u0119kszy\u0107 produktywno\u015b\u0107 i kreatywno\u015b\u0107.</p> </li> <li> <p>Edukatorzy zainteresowani zrozumieniem mo\u017cliwo\u015bci AI.</p> </li> <li> <p>Osoby kreatywne (pisarze, arty\u015bci, projektanci itp.), kt\u00f3rzy chc\u0105 wykorzystywa\u0107 AI jako kreatywnego partnera.</p> </li> <li> <p>Ka\u017cdy, kto chce poprawi\u0107 swoje umiej\u0119tno\u015bci komunikacji z systemami AI, aby uzyska\u0107 lepsze i bardziej niezawodne rezultaty.</p> </li> </ul> <p>M\u00f3wi\u0105c kr\u00f3tko \u2014 je\u015bli chcesz wykorzysta\u0107 potencja\u0142 nowoczesnej sztucznej inteligencji, ten tutorial jest w\u0142a\u015bnie dla Ciebie!</p> <p>Czy potrzebuj\u0119 do\u015bwiadczenia w programowaniu?</p> <p>Absolutnie nie! Nie jest wymagana \u017cadna wiedza z zakresu programowania ani kodowania.</p> <p>Skupiamy si\u0119 na tworzeniu skutecznych prompt\u00f3w przy u\u017cyciu j\u0119zyka naturalnego \u2014 takiego, jakim ju\u017c si\u0119 pos\u0142ugujesz na co dzie\u0144. Nie musisz by\u0107 data scientistem ani in\u017cynierem uczenia maszynowego. Ka\u017cdy mo\u017ce napisa\u0107 prompt!</p>"},{"location":"pl/Tutorials/Prompt_Engineering/For_Everyone/#chociaz-techniczne-doswiadczenie-nie-jest-wymagane-kilka-rzeczy-moze-okazac-sie-pomocnych","title":"Chocia\u017c techniczne do\u015bwiadczenie nie jest wymagane, kilka rzeczy mo\u017ce okaza\u0107 si\u0119 pomocnych","text":"<ul> <li>Ciekawo\u015b\u0107, kreatywno\u015b\u0107 i otwarto\u015b\u0107 na eksplorowanie nowych technologii.  </li> <li>Podstawowa znajomo\u015b\u0107 obs\u0142ugi komputera oraz co\u015b, co Michael Hartl nazywa wyrafinowaniem technicznym \u2014 czyli pewien poziom komfortu z u\u017cywaniem technologii, umiej\u0119tno\u015b\u0107 samodzielnego poszukiwania odpowiedzi i rozwi\u0105zywania drobnych problem\u00f3w technicznych.</li> </ul> Wyrafinowanie techniczne / XKCD <p>Najwa\u017cniejsze s\u0105 Twoja kreatywno\u015b\u0107 i gotowo\u015b\u0107 do eksperymentowania.</p>"},{"location":"pl/Tutorials/Prompt_Engineering/For_Everyone/getting_started/","title":"Konfiguracja pierwszego modelu LLM","text":"<p>Najlepszym sposobem na nauk\u0119 prompt engineeringu jest po prostu dzia\u0142anie i eksperymentowanie. Ten przewodnik skupia si\u0119 na praktycznych metodach pracy z du\u017cymi modelami j\u0119zykowymi (LLM), niezale\u017cnie od tego, czy uruchamiasz je lokalnie na w\u0142asnym sprz\u0119cie, czy korzystasz z rozwi\u0105za\u0144 chmurowych. Poni\u017cej znajdziesz przegl\u0105d narz\u0119dzi, platform i interfejs\u00f3w, kt\u00f3re pozwol\u0105 Ci od razu zacz\u0105\u0107 przygod\u0119 z LLM-ami.</p> <p>Nie martw si\u0119</p> <p>Spokojnie, je\u015bli jakie\u015b poj\u0119cia czy zwroty brzmi\u0105 obco, nie ma si\u0119 czym martwi\u0107. Wszystko wyt\u0142umaczymy w nast\u0119pnych rozdzia\u0142ach.</p>"},{"location":"pl/Tutorials/Prompt_Engineering/For_Everyone/getting_started/#loaklne-vs-hostowane-modele","title":"Loaklne vs. Hostowane Modele","text":""},{"location":"pl/Tutorials/Prompt_Engineering/For_Everyone/getting_started/#lokalne-modele","title":"Lokalne Modele","text":"<p>Modele lokalne dzia\u0142aj\u0105 ca\u0142kowicie na Twoim sprz\u0119cie, daj\u0105c Ci pe\u0142n\u0105 kontrol\u0119 nad prywatno\u015bci\u0105 danych i mo\u017cliwo\u015bci\u0105 dostosowania wszystkiego pod siebie. Narz\u0119dzia takie jak Ollama czy LM Studio upraszczaj\u0105 ca\u0142y proces, dzi\u0119ki nim nie musisz martwi\u0107 si\u0119 o wi\u0119kszo\u015b\u0107 ustawie\u0144, \u017ceby uruchomi\u0107 model (np. Mistral-7B lub Llama3-8B). Wystarczy kilka klikni\u0119\u0107 albo jedna linijka w terminalu (np. <code>ollama run llama2</code>), \u017ceby pobra\u0107 i uruchomi\u0107 LLM.</p> <p>Taki spos\u00f3b uruchamiania modeli \u015bwietnie sprawdza si\u0119 przy wra\u017cliwych projektach, bo dane nigdy nie opuszczaj\u0105 Twojego urz\u0105dzenia. Trzeba jednak pami\u0119ta\u0107 o ograniczeniach sprz\u0119towych \u2014 np. do dzia\u0142ania modelu 7B potrzebujesz co najmniej 8 GB RAM, a przy wi\u0119kszych modelach (np. 70B) przyda si\u0119 mocna karta graficzna lub specjalna wersja modelu z tzw. kwantyzacj\u0105.</p>"},{"location":"pl/Tutorials/Prompt_Engineering/For_Everyone/getting_started/#hostowane-modele","title":"Hostowane Modele","text":"<p>Modele hostowane, oferowane przez firmy takie jak OpenAI, Anthropic czy Google, zdejmuj\u0105 z u\u017cytkownika ca\u0142y ci\u0119\u017car zarz\u0105dzania infrastruktur\u0105. Dzi\u0119ki nim mo\u017cesz korzysta\u0107 z najnowszych modeli (np. GPT-4 czy Claude 3.7) za pomoc\u0105 API albo interfejs\u00f3w graficznych. To rozwi\u0105zanie bardzo wygodne i \u0142atwo skalowalne bo nie musisz si\u0119 martwi\u0107 o sprz\u0119t czy aktualizacje, bo tym wszystkim zajmuje si\u0119 dostawca.</p> <p>Z drugiej strony, poleganie na zewn\u0119trznych us\u0142ugach wi\u0105\u017ce si\u0119 z ryzykiem, Twoje zapytania i odpowiedzi mog\u0105 by\u0107 zapisywane lub wykorzystywane do dalszego trenowania modeli. Do tego, przy intensywnym u\u017cywaniu API lub subskrypcjach premium, koszty mog\u0105 szybko rosn\u0105\u0107.</p>"},{"location":"pl/Tutorials/Prompt_Engineering/For_Everyone/getting_started/#opcje-interfejsow-do-pracy-z-llm-ami","title":"Opcje interfejs\u00f3w do pracy z LLM-ami","text":"<p>Interfejsy przyjazne dla pocz\u0105tkuj\u0105cych, takie jak LM Studio i Jan, oferuj\u0105 intuicyjne \u015brodowiska graficzne do pracy z lokalnymi modelami. Te narz\u0119dzia wspieraj\u0105 funkcje takie jak:</p> <ul> <li>Przegl\u0105danie modeli i pobieranie ich z repozytori\u00f3w, np. Hugging Face.</li> <li>Interfejsy czatowe z zapisan\u0105 histori\u0105 rozm\u00f3w.</li> <li>Dostosowywanie parametr\u00f3w (np. temperatura, top-p sampling) generowania.</li> </ul> <p>Do wsp\u00f3\u0142pracy lub dla wielu u\u017cytkownik\u00f3w, Open WebUI oferuje samodzielnie hostowany interfejs webowy z funkcjami uwierzytelniania u\u017cytkownik\u00f3w oraz \"document retrical\" (RAG). Z kolei SillyTavern i AnythingLLM s\u0105 skierowane do bardziej zaawansowanych u\u017cytkownik\u00f3w, oferuj\u0105c systemy plugin\u00f3w do generowania obraz\u00f3w, przeszukiwania internetu i integracji z API.</p> <p>Uwaga</p> <p>Pojawia\u0142y si\u0119 pewne w\u0105tpliwo\u015bci niekt\u00f3rych lokalnie uruchamianych interfejsach. Na przyk\u0142ad LM Studio, cho\u0107 po pobraniu modelu nie \u0142\u0105czy si\u0119 z Internetem i wyra\u017anie deklaruje na stronie g\u0142\u00f3wnej: \"The app does not collect data or monitor your actions. Your data stays local on your machine\", to jednak pojawi\u0142y si\u0119 pytania dotycz\u0105ce jego warunk\u00f3w u\u017cytkowania. Warto wi\u0119c dok\u0142adnie zapozna\u0107 si\u0119 z regulaminem przed rozpocz\u0119ciem pracy z jakimkolwiek narz\u0119dziem.</p>"},{"location":"pl/Tutorials/Prompt_Engineering/For_Everyone/getting_started/#podsumowanie-od-czego-zaczac","title":"Podsumowanie: Od czego zacz\u0105\u0107?","text":"<p>Dla pocz\u0105tkuj\u0105cych, modele hostowane to najprostszy spos\u00f3b na start, nie trzeba nic instalowa\u0107 ani konfigurowa\u0107. Platformy takie jak OpenAI cz\u0119sto oferuj\u0105 darmowe plany, kt\u00f3re \u015bwietnie nadaj\u0105 si\u0119 do nauki i test\u00f3w. Gdy ju\u017c poczujesz si\u0119 pewniej, warto rozwa\u017cy\u0107 przej\u015bcie na modele lokalne przez Ollama lub LM Studio, kt\u00f3re daj\u0105 wi\u0119cej swobody, prywatno\u015bci i oszcz\u0119dno\u015bci przy bardziej wyspecjalizowanych zastosowaniach.</p> <p>Wyb\u00f3r UI</p> <p>Je\u015bli chcesz uczy\u0107 si\u0119 razem z nami, np. jak dostosowywa\u0107 parametry LLM-\u00f3w, warto wybra\u0107 \u015brodowisko, kt\u00f3re na to pozwala. Niekt\u00f3re darmowe wersje modeli hostowanych mog\u0105 ogranicza\u0107 albo ukrywa\u0107 dost\u0119p do takich opcji. Podamy kilka narz\u0119dzi, ale pami\u0119taj, \u017ce jest to stan na moment pisania tego przewodnika, wi\u0119c przed wyborem najlepiej samodzielnie sprawdzi\u0107, co dok\u0142adnie oferuj\u0105.</p>"},{"location":"pl/Tutorials/Prompt_Engineering/For_Everyone/getting_started/#rekomendacje","title":"Rekomendacje","text":"<p>Poni\u017csze opcje s\u0105 u\u0142o\u017cone wed\u0142ug rosn\u0105cego poziomu trudno\u015bci. Zaczynamy od najprostszych w konfiguracji, a ko\u0144czymy na tych bardziej zaawansowanych, wymagaj\u0105cych nieco wi\u0119cej wiedzy technicznej.</p>"},{"location":"pl/Tutorials/Prompt_Engineering/For_Everyone/getting_started/#hostowany-interfejs-z-darmowym-dostepem","title":"Hostowany interfejs z darmowym dost\u0119pem","text":"<p>Najprostsza opcja to skorzystanie z hostowanego, darmowego interfejsu u\u017cytkownika. Popularne platformy to  ChatGPT i Claude, ale mo\u017cesz r\u00f3wnie\u017c porozmawia\u0107 z  Bielikiem przez UI. Trzeba jednak pami\u0119ta\u0107, \u017ce w chwili pisania tego przewodnika dost\u0119p do bardziej zaawansowanych funkcji zwykle wymaga p\u0142atnej subskrypcji. Je\u015bli chcesz eksplorowa\u0107 te mo\u017cliwo\u015bci bez ponoszenia koszt\u00f3w, polecamy Google AI Studio, kt\u00f3re w ramach darmowego planu udost\u0119pnia sporo zaawansowanych opcji do pracy z modelami.</p> Google AI Studio"},{"location":"pl/Tutorials/Prompt_Engineering/For_Everyone/getting_started/#lokalny-llm-z-interfejsem-graficznym","title":"Lokalny LLM z interfejsem graficznym","text":"<p>Wymagania Sprz\u0119towe</p> <p>Ta opcja wymaga mocniejszego sprz\u0119tu, najlepiej komputera z kart\u0105 graficzn\u0105 (GPU). Co prawda mo\u017cna uruchamia\u0107 mniejsze modele w wersji \"skwantyzowanej\" (czyli zoptymalizowanej pod k\u0105tem szybko\u015bci i ni\u017cszego zu\u017cycia zasob\u00f3w), ale trzeba liczy\u0107 si\u0119 z tym, \u017ce jako\u015b\u0107 odpowiedzi mo\u017ce by\u0107 nieco ni\u017csza w por\u00f3wnaniu do pe\u0142nych wersji modeli.</p> <p>Wejd\u017a na stron\u0119 LM Studio i pobierz wersj\u0119 odpowiedni\u0105 dla Twojego systemu operacyjnego. Po zainstalowaniu, uruchom aplikacj\u0119.</p> LM Studio <p>Aplikacja nie zadzia\u0142a od razu, najpierw trzeba pobra\u0107 model. Aby to zrobi\u0107, przejd\u017a do zak\u0142adki  \"Discover\" (znajdziesz j\u0105 po lewej stronie okna aplikacji), a nast\u0119pnie wybierz interesuj\u0105cy Ci\u0119 model i wersj\u0119 kwantyzacji, np. <code>Q4_K_M</code>, i kliknij pobierz. Na pocz\u0105tek polecamy mniejszy model, taki jak <code>Bielik-11B-v2.5-Instruct</code> lub <code>Mistral-7B-Instruct-v0.3-GGUF</code>, oferuje on r\u00f3wnowag\u0119 mi\u0119dzy jako\u015bci\u0105 a wymaganiami sprz\u0119towymi. Idealny do nauki i pierwszych eksperyment\u00f3w. Je\u015bli nie masz dost\u0119pu do GPU, a mimo to chcesz uruchomi\u0107 model lokalnie, mo\u017cesz spr\u00f3bowa\u0107 mniejszych modeli, takich jak <code>Bielik-4.5B-v3.0-Instruct-GGUF</code>, <code>Bielik-1.5B-v3.0-Instruct-GGUF</code> lub <code>Gemma 4B QAT</code> (Quantization Aware Training). Te modele s\u0105 zaprojektowane z my\u015bl\u0105 o wi\u0119kszej wydajno\u015bci i mog\u0105 dzia\u0142a\u0107 nawet na komputerach bez karty graficznej, cho\u0107 ich szybko\u015b\u0107 i p\u0142ynno\u015b\u0107 odpowiedzi b\u0119d\u0105 zale\u017ce\u0107 od mo\u017cliwo\u015bci Twojego sprz\u0119tu.</p> Wyszukaj i pobierz model w LM Studio <p>Po zako\u0144czeniu pobierania przejd\u017a do zak\u0142adki  \"Chat\", a nast\u0119pnie w g\u00f3rnej cz\u0119\u015bci ekranu czatu wybierz model z rozwijanej listy (\u201eSelect a model to load\u201d), aby go za\u0142adowa\u0107 i rozpocz\u0105\u0107 rozmow\u0119.</p> Za\u0142adowanie modelu <p>i mo\u017cesz z nim rozmawia\u0107.</p> Interkacja z LLM <p>Po zako\u0144czeniu pracy pami\u0119taj, aby odmontowa\u0107 (eject) model i zamkn\u0105\u0107 aplikacj\u0119. Dzi\u0119ki temu zwolnisz zasoby systemowe i unikniesz niepotrzebnego obci\u0105\u017cenia sprz\u0119tu.</p>"},{"location":"pl/Tutorials/Prompt_Engineering/For_Everyone/getting_started/#interfejs-przez-openrouter-darmowe-i-patne-llm-y","title":"Interfejs przez OpenRouter \u2014 darmowe i p\u0142atne LLM-y","text":"<p>Aby korzysta\u0107 z Jan.ai (lub podobnej platformy, np. AnythingLLM) z OpenRouterem, potrzebne s\u0105 dodatkowe kroki konfiguracyjne:</p> <ul> <li>Pobierz i zainstaluj Jan.ai z oficjalnej strony.</li> <li>Zarejestruj si\u0119 na OpenRouter.</li> <li>Po zalogowaniu si\u0119 na stronie OpenRouter przejd\u017a do Settings \u2192 API Keys.</li> <li>Wygeneruj nowy klucz API i od razu go skopiuj, b\u0119dzie wygl\u0105da\u0142 mniej wi\u0119cej tak: <code>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</code>.</li> </ul> <p>API Key</p> <p>Klucz API mo\u017cesz zobaczy\u0107 tylko raz \u2014 je\u015bli zapomnisz go skopiowa\u0107, po prostu wygeneruj nowy.</p> <ul> <li>W aplikacji Jan.ai przejd\u017a do Settings \u2192 Remote Engine \u2192 OpenRouter i wklej sw\u00f3j klucz API w odpowiednie pole. Dzi\u0119ki temu aplikacja b\u0119dzie mog\u0142a korzysta\u0107 z modeli udost\u0119pnianych przez OpenRouter.</li> <li>Teraz mo\u017cesz rozpocz\u0105\u0107 nowy w\u0105tek i wybra\u0107 model, klikaj\u0105c selektor modeli na dole okna czatu. Wybierz Cloud \u2192 wpisz \u201efree\u201d, a nast\u0119pnie przewi\u0144, aby znale\u017a\u0107 OpenRouter i wybra\u0107 sw\u00f3j model. Pami\u0119taj, aby zapozna\u0107 si\u0119 z warunkami korzystania z modelu, poniewa\u017c niekt\u00f3re darmowe modele s\u0105 dost\u0119pne tylko dla u\u017cytkownik\u00f3w, kt\u00f3rzy dokonali wcze\u015bniej jakiegokolwiek zakupu.</li> </ul> <p>Darmowe modele</p> <p>Mo\u017cesz np. wybra\u0107 model: <code>nvidia/llama-3.3-nemotron-super-9b-v1:free</code></p> <p>Opcjonalnie \u2013 Korzystanie z modeli Google za darmo: Je\u015bli wolisz korzysta\u0107 z modeli Google, mo\u017cesz uzyska\u0107 darmowy klucz API, klikaj\u0105c \u201eGet API Key\u201d na g\u00f3rze interfejsu Google AI Studio. Nast\u0119pnie post\u0119puj zgodnie z tymi samymi krokami, ale pami\u0119taj, aby wybra\u0107 Google jako dostawc\u0119, a nie OpenRouter.</p> Jan"},{"location":"pl/Tutorials/Prompt_Engineering/For_Everyone/getting_started/#lokalne-modele-dla-zaawansowanych-uzytkownikow","title":"Lokalne modele dla zaawansowanych u\u017cytkownik\u00f3w","text":"<p>Je\u015bli chcesz u\u017cywa\u0107 LLM bezpo\u015brednio z terminala, korzystaj\u0105c z narz\u0119dzi takich jak cURL lub z poziomu j\u0119zyka programowania, na przyk\u0142ad Python, mo\u017cesz zapozna\u0107 si\u0119 z dodatkowymi metodami w sekcji Inference and Serving.</p>"},{"location":"pl/Tutorials/Prompt_Engineering/For_Everyone/getting_started/#wiecej-opcji","title":"Wi\u0119cej Opcji","text":"<p>Mo\u017cesz sprawdzi\u0107 wi\u0119cej opcji interfejs\u00f3w u\u017cytkownika, silnik\u00f3w inferencyjnych i darmowych API, takich jak OpenRouter, przechodz\u0105c do LLM Resources</p>"},{"location":"pl/Tutorials/Prompt_Engineering/For_Everyone/getting_started/#co-dalej","title":"Co dalej?","text":"<p>Eksperymentuj z LLM, kt\u00f3ry w\u0142a\u015bnie zainstalowa\u0142e\u015b, a nast\u0119pnie przejd\u017a do kolejnej sekcji.</p>"},{"location":"pl/Tutorials/Prompt_Engineering/For_Everyone/getting_started/#bonus","title":"Bonus","text":"<p>Je\u015bli masz ochot\u0119 na odrobin\u0119 nostalgii i zabawy, mo\u017cesz zainstalowa\u0107 Clippy\u2019ego, za\u0142adowa\u0107  Bielika (format GGUF np. <code>speakleash/Bielik-4.5B-v3.0-Instruct-GGUF</code> - je\u017celi pobra\u0142e\u015b go przy pomocy LM Studio ju\u017c znajduje si\u0119 na Twoim dysku) i porozmawia\u0107 z nim za pomoc\u0105 interfejsu. Mi\u0142ej zabawy!</p> Clippy w akcji"}]}